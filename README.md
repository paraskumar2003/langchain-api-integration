# FastAPI GPT Evaluation Service

A **FastAPI-based backend service** that uses **OpenAI (via LangChain)** to generate structured responses and evaluate user-submitted answers for music theory and visual reasoning questions.

This project exposes REST APIs for:

- Generating a **daily summary** in strict JSON format
- Evaluating **text, image, or audio-based questions** using GPT

---

## ğŸš€ Features

- âš¡ Built with **FastAPI**
- ğŸ¤– OpenAI integration via **LangChain**
- ğŸ“¦ Strict JSON output parsing
- ğŸ¼ Supports **music theory & visual reasoning** evaluations
- ğŸ§  Deterministic evaluation (temperature = 0)
- ğŸ›¡ï¸ Robust fallback parsing for malformed model output
- ğŸ” Environment-based API key configuration

---

## ğŸ§± Project Structure

```text
app/
â”œâ”€â”€ api/
â”‚   â””â”€â”€ index.py            # API routes
â”œâ”€â”€ models/
â”‚   â””â”€â”€ index.py            # Pydantic request/response models
â”œâ”€â”€ service/
â”‚   â””â”€â”€ index.py            # GPT interaction & evaluation logic
â”œâ”€â”€ prompts/
â”‚   â””â”€â”€ index.py            # Prompt templates
â”œâ”€â”€ parser/
â”‚   â””â”€â”€ json_parser.py      # Strict JSON output parser
â”œâ”€â”€ main.py                 # FastAPI app entry
â””â”€â”€ .env
```

---

## ğŸ› ï¸ Tech Stack

- **Python 3.10+**
- **FastAPI**
- **LangChain**
- **OpenAI API**
- **Pydantic**
- **dotenv**

---

## ğŸ”‘ Environment Setup

Create a `.env` file in the project root:

```env
OPENAI_API_KEY=your_openai_api_key_here
```

---

## ğŸ“¦ Installation

```bash
git clone https://github.com/your-username/fastapi-gpt-evaluator.git
cd fastapi-gpt-evaluator

python -m venv venv
source venv/bin/activate   # Linux / Mac
venv\Scripts\activate      # Windows

pip install -r requirements.txt
```

---

## â–¶ï¸ Running the Server

```bash
uvicorn app.main:app --reload
```

Server will start at:

```
http://127.0.0.1:8000
```

Swagger docs:

```
http://127.0.0.1:8000/docs
```

---

## ğŸ“¡ API Endpoints

---

### ğŸ—“ï¸ Generate Todayâ€™s Summary

```http
POST /chat
```

#### Request Body

```json
{
  "prompt": "Tell me about today's festivals in India"
}
```

#### Response

```json
{
  "date": "2025-12-19",
  "festivals": ["Festival A", "Festival B"],
  "summary": "Today is marked by cultural celebrations."
}
```

---

### ğŸ§  Evaluate Question Response

```http
POST /question
```

#### Request Body

```json
{
  "question": {
    "dimension": "visual",
    "level": "basic",
    "type": "written",
    "prompt_html": "Look at this 4-bar melody in C major. Count how many notes move by step.",
    "image_url": "https://example.com/image.png",
    "audio_url": null,
    "options": null,
    "response_type": "text",
    "response_text": "6",
    "response_file_url": null
  }
}
```

#### Response

```json
{
  "is_correct": true,
  "reason": "Correct answer!",
  "confidence": 0.95
}
```

---

## ğŸ§  Evaluation Logic

- **Text responses** are evaluated strictly using the submitted text
- **File-based responses** (image/audio) are evaluated using the provided URL
- Model output is required to be **strict JSON**
- Includes fallback parsing to handle malformed responses
- Confidence values are normalized between `0.0` and `1.0`

---

## ğŸ›¡ï¸ Error Handling

- Missing `question` field â†’ `400 Bad Request`
- Invalid model output â†’ safe defaults applied
- JSON parsing failures handled gracefully

---

## ğŸ“ˆ Extensibility

This project is designed to be extended for:

- Multi-question evaluations
- Rubric-based scoring
- Confidence calibration
- Database persistence
- Authentication & rate limiting
- Async file ingestion (image/audio)

---

## ğŸ“„ License

MIT

---

## â­ Notes

- This service is **backend-only**
- Designed for **machine-to-machine** interaction
- Not intended to stream responses
- Uses deterministic GPT settings for evaluation consistency
